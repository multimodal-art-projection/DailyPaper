<h1 align="center"><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg" width="25"> M-A-P Daily Paper</h1>
<p align="center">
<a href="https://github.com/DenverCoder1/readme-typing-svg"><img src="https://media.giphy.com/media/Rn26lWjqA0uUU/giphy.gif" width="100"></a>
</p>
<hr/>
<h4 align="center">The <a href=https://m-a-p.ai>M-A-P</a> daily paper project curates and reviews a selection of new papers published daily on arXiv, providing insightful commentary on cutting-edge research across various scientific disciplines.</h4>
<br>

[Click to view previous selection](https://m-a-p.ai/DailyPaper/archived_papers.html).

<hr/>

## üî• Paper Today: 17/10/2024

<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities | The researcher recommends their OmniBench and shares insights gained from three to four months of dedicated evaluation experience. They suggest that insightful evaluations should naturally possess certain characteristics in data organization: 1) Avoid coupling between model capabilities, focusing on issues that can be improved through architecture or data but may be blind spots. 2) Design evaluation sets for genuinely universal problems, including future issues, but not non-universal ones. 3) Cognitive science concepts are crucial, offering many valuable evaluation perspectives. 4) Evaluations should not fear being solved quickly; benchmarks are consumables designed to guide model improvements, fulfilling their historical mission by identifying issues. |
| Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance | The researcher highly endorses this direction, viewing it as an area where academia can continually accumulate value with measurable outputs from investments. The most valuable general insights involve LLMs' basic abilities, proactively seeking information and simple Agent Instruct capabilities. By continuously summarizing and forming general workflow paths to solve problem types, robust synthetic environments can generate numerous workflows with potential practical value for platforms like Coze or Difny. However, the researcher suggests that while generating workflows is valuable, dedicating efforts to reinforcement learning might be unnecessary at this stage. |
| PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking | The researcher finds this paper, written by an MIT PI in materials science, interesting. It presents an O1-like system concept with several insightful elements, such as using ORPO for iteration, dynamic knowledge graphs, and the notion of thinking tokens, which might serve a function similar to tone words in O1. The researcher humorously adds a citation review: "Identified as aspiring for a Nobel Prize in Physics." |
| Revealing the Barriers of Language Agents in Planning | The researcher acknowledges Blocksworld and TravelPlanner as excellent datasets, suggesting the inclusion of TravelPlanner in the OOD bench due to its wide recognition. Key takeaways align with their internal findings: current open-source models tend to follow instruction intent coarsely rather than decomposing and strictly adhering to complex composite instructions, struggling with combinations. This is identified as a key capability requiring attention. The definitions of episodic memory and parametric memory are also noted as interesting. |
| JudgeBench: A Benchmark for Evaluating LLM-based Judges | The researcher views this as a correct direction, leaning more towards Factuality and Complex Reasoning compared to RewardBench. It is seen as an effective supplement for analyzing Reward Modeling effects. |
| DDIL: Improved Diffusion Distillation With Imitation Learning | - |
| MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router | - |
| Open Domain Question Answering with Conflicting Contexts | The researcher identifies this as a valuable Reasoning Benchmark that incorporates Conflict Context. |
| Understanding the Role of LLMs in Multimodal Evaluation Benchmarks | The researcher acknowledges MMMU's limitations, noting that most questions can be answered without vision-related world knowledge. This insight is deemed important for benchmark design, emphasizing the need to strictly consider whether visual input is necessary, whether spatial awareness is required, or if text descriptions suffice. Three levels of benchmarks are proposed: no visual need, visual need without spatial awareness, and essential spatial awareness. |
| HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks | The researcher recommends integrating this multimodal coding benchmark into internal visual benchmarks. |
| Is Complex Query Answering Really Complex? | The researcher finds the paper's breakdown of partial reasoning queries versus full reasoning queries significant for language model benchmarks. They suggest that applying this decomposition to problems like HotpotQA could yield valuable insights into the proportion of reasoning versus memorized components in long chain-of-thought solutions. |
| WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines | - |
| A Scalable Communication Protocol for Networks of Large Language Models | The researcher distills the core insight: frequent communications are handled through traditional protocols, infrequent ones through structured data exchange, and rare communications use natural language. They agree with this approach, advocating for systems that utilize LLMs appropriately without assigning unnecessary active roles for the sake of being fancy. |
| Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information | The researcher sees potential in adapting this as an indicator for controlling SFT data distribution. They suggest exploring methods from multi-task learning task number scaling for potentially useful approaches. |
| Counterfactual Generative Modeling with Variational Causal Inference | - |
| Exploring Model Kinship for Merging Large Language Models | The researcher finds this paper interesting, defining a new direction in model kinship. They suggest broader applications beyond model merging and note areas for improvement, such as considering inter-layer differences and robustness across different architectures. They propose extending the concept to define cross-model kinship based on semantic similarities of head patterns and activation patterns in complex reasoning cases. |
| Rethinking Visual Counterfactual Explanations Through Region Constraint | The researcher marks this for later reading, noting that the Automated Region Extraction step seems non-essential and abrupt. They find other parts of the definition self-consistent based on their limited diffusion knowledge and plan to study it more carefully later. |
</table>


## üõ†Ô∏è Papers This Week 

(Expand to View)
<details>
<summary> <b>15/10/2024</b> </summary>

<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs | TMGBENCH includes 144 types of games based on Robinson-Goforth topologies, each containing multiple instances. These games can be further organized into sequential, parallel, and nested forms. Evaluation metrics are designed, effectively reflecting fluid intelligence and dynamic scalability. Similar to previous benchmarks, a decisive gap between open-source models and models like Claude and GPT-4 was observed. Forwarded to @Peng Tao @Yang Guang. Model behavior traits may be critical, as BoN's ability to select good cases depends on sufficient sampling diversity. However, open-source models lack dynamic thinking, possibly making inference scaling harder for these models. Highly recommended for reading, with an emphasis on improving dynamic pattern composition. |
| STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with FeedBack | A reverse thinking approach, where instead of using Actor-Critic to modify LLM, it is used to modify KB. This makes a lot of sense. Such reverse thinking seems much needed. |
| Embedding Self-Correction as an Inherent Ability in Large Language Models for Enhanced Mathematical Reasoning | Developed by Tencent, this combines OpenCodeInterpreter with self-improvement for math, though it's fairly average. Many methods introducing strong verifiers to support CoT already exist. |
| AFlow: Automating Agentic Workflow Generation | Personally, I'm not fond of using an agent workflow to solve all problems, but this paper feels like an exception with fundamental value. The abstraction suggests that if a specific workspace definer exists, it could sequentially sample and generate workspace descriptions. This has high theoretical value for frameworks like Coze and Difny. Recommended for follow-up. |
| VideoAgent: Self-Improving Video Generation | This tackles video generation with self-improvement based on external feedback, though it was only tested in robotic scenarios using MetaWorld, iTHOR, and BridgeData V2 datasets. Limited knowledge in this field makes it hard to confirm whether the title might be an overclaim, as "AIGC" comes to mind. |
| OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models | A framework with extensive support according to the paper, including PRM support for scoring and strategy selection. Key formalizations are generalizable, with few unconventional techniques. Its practical usability may depend on ease of use. Worth exploring. |
| Alignment Between the Decision-Making Logic of LLMs and Human Cognition: A Case Study on Legal LLMs | An evaluation framework for step-wise CoT generation quality, which defines irrelevant, incorrect, and correct tokens. It could extend to a framework for evaluating BoN/MCTS/CoT performance by measuring step repetition, CoT reliability, and search space. The interaction framework feels unnecessary. |
| Mechanistic? | A primer on mechanical interpretability, offering a historical overview. It could be a worthwhile read for those interested in the basics. |
| Zero-shot Commonsense Reasoning over Machine Imagination | Involves generating text QA pairs from a knowledge base, followed by text-to-image generation for a VQA dataset. An intern's project that aligned CLIP embeddings but seemed scooped. Though it improves commonsense reasoning, the work isn't particularly solid. Issues with SFT and pretraining data alignment are worth further exploration. |
| Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation | Reads like a science fiction story, but it's quite fun. The idea of generating a digital researcher and simulating collaborative research sounds interesting. It raises curiosity about personal digital researcher performance. |
| HART: Efficient Visual Generation with Hybrid Autoregressive Transformer | MIT's autoregressive diffusion-based image generation model. Strongly recommended for a detailed read. |
| LVD-2M: A Long-take Video Dataset with Temporally Dense Captions | Congrats! This synthetic long video generation data is highly valuable. |
| TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models | A valuable benchmark for video understanding. However, the videos might be too short, with the longest ones under 10 minutes. The study found that performance saturates at 8-16 frames, which is a key takeaway. Recommended reading. |
| On Information-Theoretic Measures of Predictive Uncertainty | Haven't had the chance to study the formulas thoroughly. Marked for later review. Transformation between Equation 1 and 5 seems fine. These measures might be critical for improving preference data efficiency. |
| When Attention Sink Emerges in Language Models: An Empirical View | Highly recommended reading. Although the sample size isn't large, the experiments are rigorous. Key takeaways: (1) weight decay encourages attention sinks, (2) larger training datasets exacerbate attention sinks, (3) random and repeated sequences influence attention sinks. This supports sentence-level deduplication and the removal of random sequences. |
| Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models | Simple and effective method using residuals and decoupling high-resolution adaptation to solve reconstruction accuracy issues. The approach feels like broadening the information bottleneck. Highly recommended for reading. |
| SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators | Training-free model compression method without calibration data. Introduces bypass parameters to reconstruct weight blocks, aligned with MoE fine-grained compression thoughts. |
| Adapt-‚àû: Scalable Lifelong Multimodal Instruction Tuning via Dynamic Data Selection | The idea of dynamically selecting and pooling new SFT data is a good system demo concept, although it feels underdeveloped. Recommended reading. |
| BookWorm: A Dataset for Character Description and Analysis | Potentially important for role-playing, containing character descriptions and in-depth analysis with high confidence. |
| Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective | An interesting new benchmark that measures the causal effects of semantic variation in text-to-image models. The key takeaway: cross-modal alignment in UNet or Transformers is critical, and text encoder capability isn't the sole determining factor. |
| Predicting from Strings: Language Model Embeddings for Bayesian Optimization | GDM's exploratory work using language model embeddings for Bayesian optimization. Larger models perform better, showing higher capacity and better inductive bias. |
| LOBG: Less Overfitting for Better Generalization in Vision-Language Model | Through fine-grained filtering and hierarchical logic distillation, this study improves vision-language model generalization. Although there is high exploratory value, the approach feels basic. |
| Œ±-DPO: Adaptive Reward Margin is What Direct Preference Optimization Needs | Marked for formula review. |
| FormalAlign: Automated Alignment Evaluation for Autoformalization | Addressing formalization issues in lean translation. Current metrics lack persuasiveness. Semantic analysis in this context is questionable. |
| Scalable Multi-Domain Adaptation of Language Models using Modular Experts | An early 2023 project on modular expert systems with a very rough implementation. It aligns with the idea of training multiple experts and using a routing system for unseen tasks. The approach has potential for edge-side research. |
| Collu-Bench: A Benchmark for Predicting Language Model Hallucinations in Code | A valuable benchmark addressing hallucination in code generation. The key takeaway: keywords, identifiers, and type identifiers are most prone to hallucination. |
| Retrieval Instead of Fine-tuning: A Retrieval-based Parameter Ensemble for Zero-shot Learning | Similar to another paper, it offers a retrieval-based parameter ensemble for zero-shot learning. |
| Gradient-Free Neural Network Training on the Edge | A gradient-free training method. A quick glance shows that gradient replacement involves an intermediate tensor. |
| MoIN: Mixture of Introvert Experts to Upcycle an LLM | Similar to a previous paper, exploring introvert experts for LLM. |
| Can In-context Learning Really Generalize to Out-of-distribution Tasks? | Solid experiments but similar to a previous paper, showing ICL mainly retrieves implicit functions from pretraining. |
| Reconstructive Visual Instruction Tuning | By introducing reconstructive visual instruction tuning, this significantly improves fine-grained understanding in LMMs and reduces hallucination. It aims to balance low-level visual information, though it feels somewhat esoteric. |
| Boosting Deductive Reasoning with Step Signals In RLHF | - |
| MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models | Potentially a good benchmark for evaluating fluid intelligence. |
| Fine-grained Attention I/O Complexity: Comprehensive Analysis for Backward Passes | A detailed analysis of attention mechanism I/O complexity. Worth revisiting. |

</table>

</details>

<hr/>

<details>
<summary> <b>14/10/2024</b> </summary>

<table class="center">

| Paper | Affiliation | Comments |
|:-------------|:-------------|:-------------|
| Editing Massive Concepts in Text-to-Image Diffusion Models | HKU, THU | Editing Massive Concepts in Text-to-Image Diffusion Models addresses the problem of scalable batch image editing based on concepts. However, the approach does not appear to be particularly robust. The authors have collected 1,000 potentially problematic concepts, which are meaningful, but there is uncertainty regarding how this method is intended to be applied in practical scenarios. To effectively avoid issues such as copyright infringement, bias, or factual inaccuracies in generated outputs, the model first needs to recognize when an error has occurred before it can take steps to correct it. This is the critical challenge. Diffusion-based models do not seem to possess a strong concept-based understanding of the world. The current solution appears to rely on continuously patching problems, which is not a sustainable long-term strategy. However, the approach could be useful in preventing the generation of copyrighted images. That being said, ideally, models should not be trained on copyrighted images in the first place. Additionally, the paper does not provide experimental validation for the issue of model collapse. There is room for the method to be more rigorous. The ICEB (Image Concept Editing Benchmark) used to evaluate concept-based image editing is a valuable contribution‚Äîsuch a large-scale benchmark has not been seen before. |
| Promptly Yours? A Human Subject Study on Prompt Inference in AI-Generated Art | University of Oklahoma, University of Texas | Figures 11-14 are particularly interesting. They highlight that diffusion models haven't generalized well from the original prompts to the generated images. In fact, neither humans nor AI are able to recall the original prompts accurately. |
| KV Prediction For Improved Time to First Token | Apple | Apple has recently released several intriguing works. One of them uses a small model to predict approximate values for the KV-cache of a larger model. There was a previous proposal, which wasn‚Äôt implemented, that suggested using a small model to predict which experts in a Mixture of Experts (MoE) model might activate based on the input tokens. However, it was reconsidered due to the time cost of loading and unloading experts. The advantage of this approach would be the potential to use a larger MoE model with limited GPU memory. Furthermore, it is unclear whether the KVP-C and KVP-LP methods suggest that models of different sizes, trained on the same data, learn robust activation patterns. Even after pruning or when working with models of varying sizes, the activation patterns seem to remain largely consistent. |
| UNIQ: Offline Inverse Q-learning for Avoiding Undesirable Demonstrations | Singapore Management University | The writing is solid, and the motivation is clear. The issue of sparse expert data in offline imitation learning is real. Instead of minimizing the distance to expert data, the paper proposes maximizing the distance between undesirable demonstrations, which seems like a straightforward idea, particularly for RLHF (Reinforcement Learning from Human Feedback). It‚Äôs unclear if this has been explored before; a more thorough survey could be done in the future, along with revisiting the formulas. It's worth marking for further investigation. |
| Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning? | Google Research | This work is a well-executed study on mechanical interpretability using synthetic data. However, it suffers from a common issue in such research‚Äîthe lack of in-depth data analysis, as most of the conclusions are qualitative rather than quantitative. The qualitative finding is that a single-layer Transformer can express multi-step algorithms, which is extended to show that multi-layer Transformers can learn multi-step algorithms. While this is a solid contribution, it doesn't provide a deep understanding of how multi-step algorithms are reflected in the model's parameters. Additionally, the paper demonstrates generalization to out-of-distribution (OOD) algorithms. The model was trained with data using a unit covariance matrix but tested with different covariance matrices, and the looped transformers still managed to achieve low loss, indicating strong generalization across distributions. This raises a hypothesis that pre-training may also involve learning many such algorithmic loops, where multi-hop reasoning could be seen as a form of algorithm. If these parts of the model parameters could be activated‚Äîsuch as in cases where data encoded as code has learned a divide-and-conquer method‚Äîthere‚Äôs potential for such reasoning to generalize to broader data. However, this would require the correct activation of attention heads without overfitting to input patterns. This is recommended reading. |
| Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content | Kuaishou Technology, Shenzhen University, Tsinghua University | This appears to be a highly valuable video dataset, and it seems a significant amount of money was invested in it. Further research is recommended. |
| Baichuan-Omni Technical Report | Baichuan Inc, Westlake University | Baichuan continues to justify its work in pre-training, which appears somewhat basic. The text performance is underwhelming. However, there could be some valuable insights regarding how the encoder is handled. The use of a projector following the visual encoder might improve the efficiency and effectiveness of converting visual input into semantic tokens, and it seems reasonable overall. Stage 2 involves adding synthetic QA and high-quality OCR, which is in line with how others retrain CLIP. If this approach could replace the need to retrain CLIP and caption models, it would be quite valuable. The audio method, which seems to be based on the early understanding of audio being processed similarly to images, is less convincing. It‚Äôs disappointing that the paper didn‚Äôt test on OmniBench. |
| SimpleStrat: Diversifying Language Model Generation with Stratification | UC Berkeley | This work provides a quantitative assessment of whether models can respond diversely from multiple perspectives. They designed a benchmark, ConvergeQA, which shows an average of 28 possible answers. This could help verify if a model tends to favor depth-first and fixed patterns or if it is more inclined toward exploration. It seems like an interesting observation benchmark and is recommended for further reading. |
| Agents Thinking Fast and Slow: A Talker-Reasoner Architecture | Google DeepMind | Google‚Äôs agent framework seems unremarkable, with no groundbreaking metrics. However, the behavioral pattern of a ‚Äútalker‚Äù directing a ‚Äúreasoner‚Äù to verify each intermediate step has been evident in earlier work, such as HotpotQA, Collie, AIME, Usaco, and LiveCodeBench. Collie is an outlier, where, in 50 sampled examples, no pre-planned thought was observed, while the other four benchmarks displayed consistent patterns of divide-and-conquer (COT) or retrieving potential pre-existing solutions (UKM). The intriguing part of this experiment is the high degree of convergence in thought patterns within each benchmark. The question remains whether the model learned this behavior or if it simply reflects rigid synthetic data. The latter seems more likely. For instance, in USACO and LiveCodeBench, divide-and-conquer appeared far more frequently than retrieval-based thinking (UKM), even though template-based approaches are common for humans. Additionally, the consistency of thought patterns within each benchmark supports this hypothesis. Returning to this paper, the talker-reasoner approach could potentially represent a fixed pattern in generating similar data in models like o1. |
| ‚àÄuto‚àÉ‚à®‚àßL: Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks | Arizona State University | This could serve as a scalable fluid intelligence benchmark. |
| NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models | - | This research explores controlling behavior by manipulating attention heads during inference. It‚Äôs an intriguing concept, though the use of this method to address multiple-choice questions (MCQs) feels a bit underwhelming. It‚Äôs disappointing that domestic researchers are focusing on leaderboard-hacking; at the very least, MMLU should have been used as a benchmark. |
| The Structure of the Token Space for Large Language Models | American University | The paper argues that token subspace is a stratified manifold rather than a manifold. However, the experiments do not seem particularly robust. It‚Äôs unclear what practical significance this conclusion has, so it‚Äôs worth marking for future consideration. |
| Towards Cross-Lingual LLM Evaluation for European Languages | TU Dresden, Fraunhofer IAIS | This is a benchmark collection for European minor languages. |
| CryoFM: A Flow-based Foundation Model for Cryo-EM Densities | Bytedance Research | - |
| VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding | Tsinghua University | This is a challenging benchmark for fine-grained video moment retrieval, refining coarse-grained image descriptions into those with more precise detail. It increases the level of difficulty in matching descriptions to specific frames. |
| PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning | Renmin University of China, Anthropic | It‚Äôs surprising to see a paper co-authored by Anthropic and Renmin University. The three key takeaways are intriguing: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There is a log-linear relationship between the effects of the attack and the data poison ratio; (3) Data poisoning effects can generalize to triggers not included in the poisoned data. Recommended reading. |
| On the Token Distance Modeling Ability of Higher RoPE Attention Dimension | Tsinghua University, Tencent Inc | This is a valuable read that analyzes the contribution of various RoPE dimensions to attention heads, identifying positional heads. Figure 9 shows that the top 10% of heads are active, and masking them leads to greater loss than masking the top 5%. This is worth considering in relation to the sparsity of head activation during LLM inference. For long-text cases, it might be necessary to activate more heads. The analysis reveals that the high-dimensional components of most attention heads contribute more to the attention score. The method for extending attention distribution in long texts is worth further exploration. |
| ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression | Zhejiang University, Shanghai AI Laboratory | This paper discusses dynamically determining the proportion of tokens based on attention scores during the pre-filling stage, using only important tokens. It‚Äôs seamlessly compatible with existing frameworks. This direction could be highly relevant for long video understanding, as there is often a lot of redundant information. Further research is warranted. |
| Scaling Laws for Predicting Downstream Performance in LLMs | University of Illinois Urbana-Champaign, Amazon | The solution does not directly optimize for the inability to fit downstream performance, and the understanding of downstream datasets' distribution is limited. Section 5.1‚Äôs formula is worth investigating. One promising direction is how to segment and fine-tune pre-training data to save on experimental costs. It references a comment: ‚ÄúFrom an efficiency perspective during inference time, using MLP or a more advanced model for regression would be a better choice; lightgbm + simulation has some flaws.‚Äù Fine-tuning learning rate and data schedulers could also be valuable. The paper references a technique where high-quality data is upsampled during the final stages of training (the annealing phase) to boost performance without introducing new data. This approach contrasts with MiniCPM, as it only upsampled existing data but still improved results. Data mixtures might not be fixed ratios either. If unfamiliar with data mixture laws, this paper is worth reading, along with D-CPT Law, RegMix, and a recent paper from Amazon that applies this concept outside of LLMs. |


</table>

</details>

<hr/>

If you are intereted in the work published by us, please navigate to our [full paper list](https://huggingface.co/collections/m-a-p/m-a-p-full-paper-list-65e070a694c7b01c5547fbff).
