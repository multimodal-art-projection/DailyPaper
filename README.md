<h1 align="center"><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg" width="25"> M-A-P Daily Paper</h1>
<p align="center">
<a href="https://github.com/DenverCoder1/readme-typing-svg"><img src="https://media.giphy.com/media/Rn26lWjqA0uUU/giphy.gif" width="100"></a>
</p>
<hr/>
<h4 align="center">The <a href=https://m-a-p.ai>M-A-P</a> daily paper project curates and reviews a selection of new papers published daily on arXiv, providing insightful commentary on cutting-edge research across various scientific disciplines.</h4>
<br>

[Click to view previous selection](https://m-a-p.ai/DailyPaper/archived_papers.html).

<hr/>

## üî• Paper Today: 15/10/2024

<table class="center">

| Paper | Affiliation | Comments |
|:-------------|:-------------|:-------------|
| Editing Massive Concepts in Text-to-Image Diffusion Models | HKU, THU | Editing Massive Concepts in Text-to-Image Diffusion Models addresses the problem of scalable batch image editing based on concepts. However, the approach does not appear to be particularly robust. The authors have collected 1,000 potentially problematic concepts, which are meaningful, but there is uncertainty regarding how this method is intended to be applied in practical scenarios. To effectively avoid issues such as copyright infringement, bias, or factual inaccuracies in generated outputs, the model first needs to recognize when an error has occurred before it can take steps to correct it. This is the critical challenge. Diffusion-based models do not seem to possess a strong concept-based understanding of the world. The current solution appears to rely on continuously patching problems, which is not a sustainable long-term strategy. However, the approach could be useful in preventing the generation of copyrighted images. That being said, ideally, models should not be trained on copyrighted images in the first place. Additionally, the paper does not provide experimental validation for the issue of model collapse. There is room for the method to be more rigorous. The ICEB (Image Concept Editing Benchmark) used to evaluate concept-based image editing is a valuable contribution‚Äîsuch a large-scale benchmark has not been seen before. |
| Promptly Yours? A Human Subject Study on Prompt Inference in AI-Generated Art | University of Oklahoma, University of Texas | Figures 11-14 are particularly interesting. They highlight that diffusion models haven't generalized well from the original prompts to the generated images. In fact, neither humans nor AI are able to recall the original prompts accurately. |
| KV Prediction For Improved Time to First Token | Apple | Apple has recently released several intriguing works. One of them uses a small model to predict approximate values for the KV-cache of a larger model. There was a previous proposal, which wasn‚Äôt implemented, that suggested using a small model to predict which experts in a Mixture of Experts (MoE) model might activate based on the input tokens. However, it was reconsidered due to the time cost of loading and unloading experts. The advantage of this approach would be the potential to use a larger MoE model with limited GPU memory. Furthermore, it is unclear whether the KVP-C and KVP-LP methods suggest that models of different sizes, trained on the same data, learn robust activation patterns. Even after pruning or when working with models of varying sizes, the activation patterns seem to remain largely consistent. |
| UNIQ: Offline Inverse Q-learning for Avoiding Undesirable Demonstrations | Singapore Management University | The writing is solid, and the motivation is clear. The issue of sparse expert data in offline imitation learning is real. Instead of minimizing the distance to expert data, the paper proposes maximizing the distance between undesirable demonstrations, which seems like a straightforward idea, particularly for RLHF (Reinforcement Learning from Human Feedback). It‚Äôs unclear if this has been explored before; a more thorough survey could be done in the future, along with revisiting the formulas. It's worth marking for further investigation. |
| Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning? | Google Research | This work is a well-executed study on mechanical interpretability using synthetic data. However, it suffers from a common issue in such research‚Äîthe lack of in-depth data analysis, as most of the conclusions are qualitative rather than quantitative. The qualitative finding is that a single-layer Transformer can express multi-step algorithms, which is extended to show that multi-layer Transformers can learn multi-step algorithms. While this is a solid contribution, it doesn't provide a deep understanding of how multi-step algorithms are reflected in the model's parameters. Additionally, the paper demonstrates generalization to out-of-distribution (OOD) algorithms. The model was trained with data using a unit covariance matrix but tested with different covariance matrices, and the looped transformers still managed to achieve low loss, indicating strong generalization across distributions. This raises a hypothesis that pre-training may also involve learning many such algorithmic loops, where multi-hop reasoning could be seen as a form of algorithm. If these parts of the model parameters could be activated‚Äîsuch as in cases where data encoded as code has learned a divide-and-conquer method‚Äîthere‚Äôs potential for such reasoning to generalize to broader data. However, this would require the correct activation of attention heads without overfitting to input patterns. This is recommended reading. |
| Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content | Kuaishou Technology, Shenzhen University, Tsinghua University | This appears to be a highly valuable video dataset, and it seems a significant amount of money was invested in it. Further research is recommended. |
| Baichuan-Omni Technical Report | Baichuan Inc, Westlake University | Baichuan continues to justify its work in pre-training, which appears somewhat basic. The text performance is underwhelming. However, there could be some valuable insights regarding how the encoder is handled. The use of a projector following the visual encoder might improve the efficiency and effectiveness of converting visual input into semantic tokens, and it seems reasonable overall. Stage 2 involves adding synthetic QA and high-quality OCR, which is in line with how others retrain CLIP. If this approach could replace the need to retrain CLIP and caption models, it would be quite valuable. The audio method, which seems to be based on the early understanding of audio being processed similarly to images, is less convincing. It‚Äôs disappointing that the paper didn‚Äôt test on OmniBench. |
| SimpleStrat: Diversifying Language Model Generation with Stratification | UC Berkeley | This work provides a quantitative assessment of whether models can respond diversely from multiple perspectives. They designed a benchmark, ConvergeQA, which shows an average of 28 possible answers. This could help verify if a model tends to favor depth-first and fixed patterns or if it is more inclined toward exploration. It seems like an interesting observation benchmark and is recommended for further reading. |
| Agents Thinking Fast and Slow: A Talker-Reasoner Architecture | Google DeepMind | Google‚Äôs agent framework seems unremarkable, with no groundbreaking metrics. However, the behavioral pattern of a ‚Äútalker‚Äù directing a ‚Äúreasoner‚Äù to verify each intermediate step has been evident in earlier work, such as HotpotQA, Collie, AIME, Usaco, and LiveCodeBench. Collie is an outlier, where, in 50 sampled examples, no pre-planned thought was observed, while the other four benchmarks displayed consistent patterns of divide-and-conquer (COT) or retrieving potential pre-existing solutions (UKM). The intriguing part of this experiment is the high degree of convergence in thought patterns within each benchmark. The question remains whether the model learned this behavior or if it simply reflects rigid synthetic data. The latter seems more likely. For instance, in USACO and LiveCodeBench, divide-and-conquer appeared far more frequently than retrieval-based thinking (UKM), even though template-based approaches are common for humans. Additionally, the consistency of thought patterns within each benchmark supports this hypothesis. Returning to this paper, the talker-reasoner approach could potentially represent a fixed pattern in generating similar data in models like o1. |
| ‚àÄuto‚àÉ‚à®‚àßL: Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks | Arizona State University | This could serve as a scalable fluid intelligence benchmark. |
| NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models | - | This research explores controlling behavior by manipulating attention heads during inference. It‚Äôs an intriguing concept, though the use of this method to address multiple-choice questions (MCQs) feels a bit underwhelming. It‚Äôs disappointing that domestic researchers are focusing on leaderboard-hacking; at the very least, MMLU should have been used as a benchmark. |
| The Structure of the Token Space for Large Language Models | American University | The paper argues that token subspace is a stratified manifold rather than a manifold. However, the experiments do not seem particularly robust. It‚Äôs unclear what practical significance this conclusion has, so it‚Äôs worth marking for future consideration. |
| Towards Cross-Lingual LLM Evaluation for European Languages | TU Dresden, Fraunhofer IAIS | This is a benchmark collection for European minor languages. |
| CryoFM: A Flow-based Foundation Model for Cryo-EM Densities | Bytedance Research | - |
| VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding | Tsinghua University | This is a challenging benchmark for fine-grained video moment retrieval, refining coarse-grained image descriptions into those with more precise detail. It increases the level of difficulty in matching descriptions to specific frames. |
| PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning | Renmin University of China, Anthropic | It‚Äôs surprising to see a paper co-authored by Anthropic and Renmin University. The three key takeaways are intriguing: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There is a log-linear relationship between the effects of the attack and the data poison ratio; (3) Data poisoning effects can generalize to triggers not included in the poisoned data. Recommended reading. |
| On the Token Distance Modeling Ability of Higher RoPE Attention Dimension | Tsinghua University, Tencent Inc | This is a valuable read that analyzes the contribution of various RoPE dimensions to attention heads, identifying positional heads. Figure 9 shows that the top 10% of heads are active, and masking them leads to greater loss than masking the top 5%. This is worth considering in relation to the sparsity of head activation during LLM inference. For long-text cases, it might be necessary to activate more heads. The analysis reveals that the high-dimensional components of most attention heads contribute more to the attention score. The method for extending attention distribution in long texts is worth further exploration. |
| ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression | Zhejiang University, Shanghai AI Laboratory | This paper discusses dynamically determining the proportion of tokens based on attention scores during the pre-filling stage, using only important tokens. It‚Äôs seamlessly compatible with existing frameworks. This direction could be highly relevant for long video understanding, as there is often a lot of redundant information. Further research is warranted. |
| Scaling Laws for Predicting Downstream Performance in LLMs | University of Illinois Urbana-Champaign, Amazon | The solution does not directly optimize for the inability to fit downstream performance, and the understanding of downstream datasets' distribution is limited. Section 5.1‚Äôs formula is worth investigating. One promising direction is how to segment and fine-tune pre-training data to save on experimental costs. It references a comment: ‚ÄúFrom an efficiency perspective during inference time, using MLP or a more advanced model for regression would be a better choice; lightgbm + simulation has some flaws.‚Äù Fine-tuning learning rate and data schedulers could also be valuable. The paper references a technique where high-quality data is upsampled during the final stages of training (the annealing phase) to boost performance without introducing new data. This approach contrasts with MiniCPM, as it only upsampled existing data but still improved results. Data mixtures might not be fixed ratios either. If unfamiliar with data mixture laws, this paper is worth reading, along with D-CPT Law, RegMix, and a recent paper from Amazon that applies this concept outside of LLMs. |


</table>


## üõ†Ô∏è Papers This Week 

(Expand to View)

<details>
<summary> <b>14/10/2024</b> </summary>

<table class="center">

| Paper | Affiliation | Comments |
|:-------------|:-------------|:-------------|
| Editing Massive Concepts in Text-to-Image Diffusion Models | HKU, THU | Editing Massive Concepts in Text-to-Image Diffusion Models addresses the problem of scalable batch image editing based on concepts. However, the approach does not appear to be particularly robust. The authors have collected 1,000 potentially problematic concepts, which are meaningful, but there is uncertainty regarding how this method is intended to be applied in practical scenarios. To effectively avoid issues such as copyright infringement, bias, or factual inaccuracies in generated outputs, the model first needs to recognize when an error has occurred before it can take steps to correct it. This is the critical challenge. Diffusion-based models do not seem to possess a strong concept-based understanding of the world. The current solution appears to rely on continuously patching problems, which is not a sustainable long-term strategy. However, the approach could be useful in preventing the generation of copyrighted images. That being said, ideally, models should not be trained on copyrighted images in the first place. Additionally, the paper does not provide experimental validation for the issue of model collapse. There is room for the method to be more rigorous. The ICEB (Image Concept Editing Benchmark) used to evaluate concept-based image editing is a valuable contribution‚Äîsuch a large-scale benchmark has not been seen before. |
| Promptly Yours? A Human Subject Study on Prompt Inference in AI-Generated Art | University of Oklahoma, University of Texas | Figures 11-14 are particularly interesting. They highlight that diffusion models haven't generalized well from the original prompts to the generated images. In fact, neither humans nor AI are able to recall the original prompts accurately. |
| KV Prediction For Improved Time to First Token | Apple | Apple has recently released several intriguing works. One of them uses a small model to predict approximate values for the KV-cache of a larger model. There was a previous proposal, which wasn‚Äôt implemented, that suggested using a small model to predict which experts in a Mixture of Experts (MoE) model might activate based on the input tokens. However, it was reconsidered due to the time cost of loading and unloading experts. The advantage of this approach would be the potential to use a larger MoE model with limited GPU memory. Furthermore, it is unclear whether the KVP-C and KVP-LP methods suggest that models of different sizes, trained on the same data, learn robust activation patterns. Even after pruning or when working with models of varying sizes, the activation patterns seem to remain largely consistent. |
| UNIQ: Offline Inverse Q-learning for Avoiding Undesirable Demonstrations | Singapore Management University | The writing is solid, and the motivation is clear. The issue of sparse expert data in offline imitation learning is real. Instead of minimizing the distance to expert data, the paper proposes maximizing the distance between undesirable demonstrations, which seems like a straightforward idea, particularly for RLHF (Reinforcement Learning from Human Feedback). It‚Äôs unclear if this has been explored before; a more thorough survey could be done in the future, along with revisiting the formulas. It's worth marking for further investigation. |
| Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning? | Google Research | This work is a well-executed study on mechanical interpretability using synthetic data. However, it suffers from a common issue in such research‚Äîthe lack of in-depth data analysis, as most of the conclusions are qualitative rather than quantitative. The qualitative finding is that a single-layer Transformer can express multi-step algorithms, which is extended to show that multi-layer Transformers can learn multi-step algorithms. While this is a solid contribution, it doesn't provide a deep understanding of how multi-step algorithms are reflected in the model's parameters. Additionally, the paper demonstrates generalization to out-of-distribution (OOD) algorithms. The model was trained with data using a unit covariance matrix but tested with different covariance matrices, and the looped transformers still managed to achieve low loss, indicating strong generalization across distributions. This raises a hypothesis that pre-training may also involve learning many such algorithmic loops, where multi-hop reasoning could be seen as a form of algorithm. If these parts of the model parameters could be activated‚Äîsuch as in cases where data encoded as code has learned a divide-and-conquer method‚Äîthere‚Äôs potential for such reasoning to generalize to broader data. However, this would require the correct activation of attention heads without overfitting to input patterns. This is recommended reading. |
| Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content | Kuaishou Technology, Shenzhen University, Tsinghua University | This appears to be a highly valuable video dataset, and it seems a significant amount of money was invested in it. Further research is recommended. |
| Baichuan-Omni Technical Report | Baichuan Inc, Westlake University | Baichuan continues to justify its work in pre-training, which appears somewhat basic. The text performance is underwhelming. However, there could be some valuable insights regarding how the encoder is handled. The use of a projector following the visual encoder might improve the efficiency and effectiveness of converting visual input into semantic tokens, and it seems reasonable overall. Stage 2 involves adding synthetic QA and high-quality OCR, which is in line with how others retrain CLIP. If this approach could replace the need to retrain CLIP and caption models, it would be quite valuable. The audio method, which seems to be based on the early understanding of audio being processed similarly to images, is less convincing. It‚Äôs disappointing that the paper didn‚Äôt test on OmniBench. |
| SimpleStrat: Diversifying Language Model Generation with Stratification | UC Berkeley | This work provides a quantitative assessment of whether models can respond diversely from multiple perspectives. They designed a benchmark, ConvergeQA, which shows an average of 28 possible answers. This could help verify if a model tends to favor depth-first and fixed patterns or if it is more inclined toward exploration. It seems like an interesting observation benchmark and is recommended for further reading. |
| Agents Thinking Fast and Slow: A Talker-Reasoner Architecture | Google DeepMind | Google‚Äôs agent framework seems unremarkable, with no groundbreaking metrics. However, the behavioral pattern of a ‚Äútalker‚Äù directing a ‚Äúreasoner‚Äù to verify each intermediate step has been evident in earlier work, such as HotpotQA, Collie, AIME, Usaco, and LiveCodeBench. Collie is an outlier, where, in 50 sampled examples, no pre-planned thought was observed, while the other four benchmarks displayed consistent patterns of divide-and-conquer (COT) or retrieving potential pre-existing solutions (UKM). The intriguing part of this experiment is the high degree of convergence in thought patterns within each benchmark. The question remains whether the model learned this behavior or if it simply reflects rigid synthetic data. The latter seems more likely. For instance, in USACO and LiveCodeBench, divide-and-conquer appeared far more frequently than retrieval-based thinking (UKM), even though template-based approaches are common for humans. Additionally, the consistency of thought patterns within each benchmark supports this hypothesis. Returning to this paper, the talker-reasoner approach could potentially represent a fixed pattern in generating similar data in models like o1. |
| ‚àÄuto‚àÉ‚à®‚àßL: Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks | Arizona State University | This could serve as a scalable fluid intelligence benchmark. |
| NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models | - | This research explores controlling behavior by manipulating attention heads during inference. It‚Äôs an intriguing concept, though the use of this method to address multiple-choice questions (MCQs) feels a bit underwhelming. It‚Äôs disappointing that domestic researchers are focusing on leaderboard-hacking; at the very least, MMLU should have been used as a benchmark. |
| The Structure of the Token Space for Large Language Models | American University | The paper argues that token subspace is a stratified manifold rather than a manifold. However, the experiments do not seem particularly robust. It‚Äôs unclear what practical significance this conclusion has, so it‚Äôs worth marking for future consideration. |
| Towards Cross-Lingual LLM Evaluation for European Languages | TU Dresden, Fraunhofer IAIS | This is a benchmark collection for European minor languages. |
| CryoFM: A Flow-based Foundation Model for Cryo-EM Densities | Bytedance Research | - |
| VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding | Tsinghua University | This is a challenging benchmark for fine-grained video moment retrieval, refining coarse-grained image descriptions into those with more precise detail. It increases the level of difficulty in matching descriptions to specific frames. |
| PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning | Renmin University of China, Anthropic | It‚Äôs surprising to see a paper co-authored by Anthropic and Renmin University. The three key takeaways are intriguing: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There is a log-linear relationship between the effects of the attack and the data poison ratio; (3) Data poisoning effects can generalize to triggers not included in the poisoned data. Recommended reading. |
| On the Token Distance Modeling Ability of Higher RoPE Attention Dimension | Tsinghua University, Tencent Inc | This is a valuable read that analyzes the contribution of various RoPE dimensions to attention heads, identifying positional heads. Figure 9 shows that the top 10% of heads are active, and masking them leads to greater loss than masking the top 5%. This is worth considering in relation to the sparsity of head activation during LLM inference. For long-text cases, it might be necessary to activate more heads. The analysis reveals that the high-dimensional components of most attention heads contribute more to the attention score. The method for extending attention distribution in long texts is worth further exploration. |
| ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression | Zhejiang University, Shanghai AI Laboratory | This paper discusses dynamically determining the proportion of tokens based on attention scores during the pre-filling stage, using only important tokens. It‚Äôs seamlessly compatible with existing frameworks. This direction could be highly relevant for long video understanding, as there is often a lot of redundant information. Further research is warranted. |
| Scaling Laws for Predicting Downstream Performance in LLMs | University of Illinois Urbana-Champaign, Amazon | The solution does not directly optimize for the inability to fit downstream performance, and the understanding of downstream datasets' distribution is limited. Section 5.1‚Äôs formula is worth investigating. One promising direction is how to segment and fine-tune pre-training data to save on experimental costs. It references a comment: ‚ÄúFrom an efficiency perspective during inference time, using MLP or a more advanced model for regression would be a better choice; lightgbm + simulation has some flaws.‚Äù Fine-tuning learning rate and data schedulers could also be valuable. The paper references a technique where high-quality data is upsampled during the final stages of training (the annealing phase) to boost performance without introducing new data. This approach contrasts with MiniCPM, as it only upsampled existing data but still improved results. Data mixtures might not be fixed ratios either. If unfamiliar with data mixture laws, this paper is worth reading, along with D-CPT Law, RegMix, and a recent paper from Amazon that applies this concept outside of LLMs. |

</table>

</details>

<hr/>

If you are intereted in the work published by us, please navigate to our [full paper list](https://huggingface.co/collections/m-a-p/m-a-p-full-paper-list-65e070a694c7b01c5547fbff).
