<h1 align="center"><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg" width="25"> M-A-P Daily Paper</h1>
<p align="center">
<a href="https://github.com/DenverCoder1/readme-typing-svg"><img src="https://media.giphy.com/media/Rn26lWjqA0uUU/giphy.gif" width="100"></a>
</p>
<hr/>
<h4 align="center">The <a href=https://m-a-p.ai>M-A-P</a> daily paper project curates and reviews a selection of new papers published daily on arXiv, providing insightful commentary on cutting-edge research across various scientific disciplines.</h4>
<br>
<hr/>

[back to main page](https://m-a-p.ai/DailyPaper)


## üõ†Ô∏è Papers This Week 

(Expand to View)

<details>
<summary> <b>13/9/2024</b> </summary>

<table class="center">


| Paper | Comments |
|:-------------|:-------------|
| A Survey of Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges | Discusses an intriguing topic in the data collection of RL simulation environments, specifically ICRL, focusing on the implicit constraints adhered to by expert agents, utilizing experience gathered from both the environment and the observed demonstration dataset. |
| The Role of Deep Learning Regularizations on Actors in Offline RL | Identifies the generalization of the Actor network as a significant bottleneck in Offline RL and explores the effects of classic Deep Learning Regularizations. The ablation study on trick ensembling is particularly interesting. |
| What Makes a Maze Look Like a Maze? | Introduces a visual Chain of Thought (CoT); Tables 1 and 2 appear to be impressive. |
| AudioBERT: Audio Knowledge Augmented Language Model | - |
| DSBench: How Far Are Data Science Agents to Becoming Data Science Experts? | Presents a challenging benchmark for structured data processing. |
| Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities | - |
| IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation | -|
| Learning Causally Invariant Reward Functions from Diverse Demonstrations | - |
| OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering | - |
| Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale | - |


</table>

</details>


<details>
<summary> <b>12/9/2024</b> </summary>
<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| Policy Filtration in RLHF to Fine-Tune LLM for Code Generation | Introduces PF-PPO and proposes the coefficient of determination (R¬≤) between rewards and actual scores on filtered samples as a metric to filter out noisy rewards in cases with longer reasoning steps. Based on OpenRLHF, it is easily adaptable. |
| What to align in multimodal contrastive learning? | Synthetic continued pretraining presents a potentially effective domain-specific data augmentation strategy involving synthetic data. The initial impression suggests it resembles the abstraction of entities, relevant descriptions, and potential relationships, while generating a significant amount of synthetic data describing potential Chains of Thought (CoT) and relationships. The generated data may resemble a more complex version of Hotpot QA. Scalability is noted, but its utility remains uncertain. Tatsunori's group frequently presents interesting ideas, with a relevant reference being a recent paper: IMPROVING PRETRAINING DATA USING PERPLEXITY CORRELATIONS. |
| Recurrent Aggregators in Neural Algorithmic Reasoning | - |
| Generative Hierarchical Materials Search | Similar to the previous comment, the two papers on GDM do not currently appear to offer any useful insights. |
| AGENT WORKFLOW MEMORY | - |
| FreeRide: Harvesting Bubbles in Pipeline Parallelism | -|
| You Have Thirteen Hours in Which to Solve the Labyrinth: Enhancing AI Game Masters with Function Calling | Interactive storytelling is a niche but intriguing field. This paper presents a straightforward approach by incorporating inherent story background elements and the mechanics of dice-rolling adventure games, which will excite role-playing game enthusiasts. Interactive novel generation is likely to attract interest from many subgroups. |
| Neural Algorithmic Reasoning with Multiple Correct Solutions | - |

</table>
</details>

<details>
<summary> <b>11/9/2024</b> </summary>
<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training Data | This paper presents a synthetic data generation practice for automatically producing code that integrates secure libraries. It appears relatively user-friendly and easy to merge. |
| Learning Generative Interactive Environments By Trained Agent Exploration | This toy work is focused on data generation in the direction of Google Genie, which currently receives limited attention, yet it is intriguing. The Genie paper mentions minimal data collection and training content. The generalization of the Decision Transformer across multiple games is also a compelling topic, with Google actively exploring this direction. Related literature includes "Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction" and "Multi-Game Decision Transformers." |
| An End-to-End Approach for Chord-Conditioned Song Generation | This work is related to SongCreator, which focuses on a similar area. |
| SongCreator: Lyrics-based Universal Song Generation | - |
| Quantifying and Enabling the Interpretability of CLIP-like Models | This work from Berkeley and Intel provides a useful interpretability library for CLIP. CLIP-InterpreT offers five types of analyses: property-based nearest neighbor search, per-head topic segmentation, contrastive segmentation, per-head nearest neighbors of an image, and per-head nearest neighbors of text. It points out that the representations learned by larger CLIP models are significantly stronger than those of relatively smaller CLIP models. |
| LLaMA-Omni: Seamless Speech Interaction with Large Language Models | This paper constructs a speech interaction dataset called InstructS2S-200k, which may have some utility. |
| Geometric-Averaged Preference Optimization for Soft Preference Labels | This work from GDM introduces distributional soft preference labels in DPO to reflect potential differences in the distributions of preferences among various annotators. This approach could be effectively integrated into various *PO families. |
| Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis | - |
| Doppelg√§nger's Watch: A Split Objective Approach to Large Language Models | This work relates to Meta's future directions. |
| Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity | This paper from Google discusses a more general modeling approach for datasets and tasks. It remains uncertain whether this can be directly adopted across different dataset subsets in LLMs. The combination experiments were replaced with independent fit experiments for each subset, and a linearized model was trained to fit the data. |
| ùïåùïä‚ÑÇùîª: Improving Code Generation of LLMs by Uncertainty-Aware Selective Contrastive Decoding | - |
| Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks | The observations in this paper are quite interesting. Both Chain-of-Thought and In-Context Learning primarily retrieve task or reasoning priors and process patterns in the input based on these priors. Research into the extent to which these priors are learned during pretraining and how they are activated during alignment and inference stages could be significant topics of study. Recommended reading includes "What Do Language Models Learn in Context? The Structured Task Hypothesis." |
| DiPT: Enhancing LLM reasoning through diversified perspective-taking | This is another approach resembling CoT+BoN that introduces the concept of perspective. |

</table>
</details>

<details>
<summary> <b>10/9/2024</b> </summary>
<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| Towards a Unified View of Preference Learning for Large Language Models: A Survey | This paper provides an overview of preference data in the context of large language models. |
| Benchmarking Chinese Knowledge Rectification in Large Language Models | Scenarios such as idioms and humor explanations may serve as excellent test subjects for large language models, particularly emphasizing the understanding of Chinese cultural metaphors. |
| MMEVOL: EMPOWERING MULTIMODAL LARGE LANGUAGE MODELS WITH EVOL-INSTRUCT | As stated in the title. |
| Semifactual Explanations for Reinforcement Learning | The introduction of a new concept of semifactual testing allows for a better understanding of the behavior of reinforcement learning agents. The "Even If" design may also be applicable to value alignment detection in language models. |
| Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small | Sparse autoencoders currently do not adequately handle causal analysis, and the conclusions drawn may not be reliable. |
| Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models | This presents a synthetic data approach for long-text continuation training, resembling needle-in-a-haystack methods and sentence order prediction. Additional recommended reading includes "Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models" and "LONGCITE: ENABLING LLMS TO GENERATE FINE-GRAINED CITATIONS IN LONG-CONTEXT QA." |
| Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models | For queries, when the responses generated by the current model are acceptable, the approach utilizes outputs closer to the base model. Otherwise, it is defined as a new skill, leading to the design of the SSR algorithm. The experiments are not sufficiently solid, although SSR claims to enhance generalization. Related reading materials include "Language Models Resist Alignment." |
| Reward-Directed Score-Based Diffusion Models via q-Learning | - | 

</table>
</details>

<details>
<summary> <b>9/9/2024</b> </summary>
<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs | This study investigates the relationship between knowledge retrieval within models and in-context learning (ICL), particularly focusing on the data efficiency of ICL examples. The authors provide interesting perspectives and conduct experiments on three toy regression datasets. The experimental results may be influenced by the atomic computational capabilities. |
| How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data | This paper presents an effective code data decontamination and code instruction data pruning approach. |
| MULTI-PROGRAMMING LANGUAGE ENSEMBLE FOR CODE GENERATION IN LARGE LANGUAGE MODEL | - |
| Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers | This paper features an interesting experimental design for human testing on complex problems, yielding intriguing results. Although many of the ideas proposed by AI are not feasible, they are notably more novel than those generated by most human researchers, especially after re-ranking. This aligns with the typical style of Professor Diyi's research group. |

</table>
</details>
<hr/>

If you are intereted in the work published by us, please navigate to our [full paper list](https://huggingface.co/collections/m-a-p/m-a-p-full-paper-list-65e070a694c7b01c5547fbff).
