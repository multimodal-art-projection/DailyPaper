<h1 align="center"><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg" width="25"> M-A-P Daily Paper</h1>
<p align="center">
<a href="https://github.com/DenverCoder1/readme-typing-svg"><img src="https://media.giphy.com/media/Rn26lWjqA0uUU/giphy.gif" width="100"></a>
</p>
<hr/>
<h4 align="center">The <a href=https://m-a-p.ai>M-A-P</a> daily paper project curates and reviews a selection of new papers published daily on arXiv, providing insightful commentary on cutting-edge research across various scientific disciplines.</h4>
<br>
<hr/>

[back to main page](https://m-a-p.ai/DailyPaper)


## üõ†Ô∏è Papers This Week 

(Expand to View)

<details>
<summary> <b>28/9/2024</b> </summary>

<table class="center">

| Paper  | Affiliation | Comments |
|:-------------|:-------------|:-------------|
| MIO: A Foundation Model on Multimodal Tokens | Beihang University, 01.AI, M-A-P | The user contributed to this paper, which focuses on joint modeling of multimodal tokens. While the paradigm isn‚Äôt particularly unique, the pretraining and supervised fine-tuning (SFT) data are among the most solid in current open-source Any2Any models. This is an initial version, and more ablation studies will be released soon. The user also recommends another ongoing project, OmniBench: Towards The Future of Universal Omni-Language Models, which explores omni-language models, a highly imaginative research direction. |
| Emu3: Next-Token Prediction is All You Need | Emu3 Team, BAAI | This is BAAI‚Äôs Any2Any model, with a few key points worth noting about the data processing: (1) Optical flaw removal eliminates frames with minimal or extreme motion during transitions, (2) supplementary data for image understanding is added in the pretraining phase, and (3) special tokens like [SOV], [SOT], and [EOV] are introduced to parse meta text and vision tokens. The training process also incorporates DPO (Direct Preference Optimization). |
| Infer Human's Intentions Before Following Natural Language Instructions | University of Washington, MIT CSAIL | This paper presents a simple intuition and method: human instructions may contain ambiguities or be inconsistent with the conveyed intention. The key takeaway is that robots should analyze what humans actually want them to do in a given scenario, incorporate this into the instructions, and then act accordingly. |
| Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective | The Hebrew University | This paper is recommended for reading. The assumptions don‚Äôt seem overly strong, and it explains from a probabilistic perspective why decomposing a problem into multiple subproblems is beneficial. The user hasn‚Äôt fully reviewed the proofs but suggests others could scrutinize it for potential errors. |
| StressPrompt: Does Stress Impact Large Language Models and Human Performance Similarly? | Institute of Automation (CAS), Beijing Institute of AI Safety and Governance | The takeaway here is that models, like humans, perform better under moderate stress, following the Yerkes-Dodson law. This type of research could be called "model behavioral psychology," and many interesting conclusions have been drawn lately, though the credibility of such findings is about 50-50. |
| Enhancing Elusive Clues in Knowledge Learning by Contrasting Attention of Language Models | Tsinghua University | This paper presents an interesting mechanism where attention distributions of smaller and larger models reading the same knowledge-intensive documents are compared. By dropping certain tokens, the model is encouraged to learn non-obvious but important clues, boosting performance. The fact that large models also benefit suggests that learning irrelevant information still affects large models. However, the experiments are not particularly convincing. |
| Explanation Bottleneck Models | NTT, Kyoto University | This paper introduces a Preliminary Concept Set Free VAE Explanation Model. |
| FactorSim: Generative Simulation via Factorized Representation | Stanford University, Nvidia | This paper is highly recommended for reading. It mimics human abilities for environmental and spatial imagination, which complements reinforcement learning (RL) methods. The user also suggests another paper from GDM collaborators two years ago titled Mind's Eye: Grounded Language Model Reasoning through Simulation. Such environments, besides code/math simulations, are easily accessible and offer great exploration potential. For example, a simulated chemistry lab where the model decides what action to perform and summarizes the results, or video games like Atari, where the model reads a tutorial and quickly learns to play. This research direction holds significant promise. |
| Just Say What You Want: Only-Prompting Self-Rewarding Online Preference Optimization | ShanghaiTech University, Northwestern University | - |
| Human Mobility Modeling with Limited Information via Large Language Models | UCLA, Novateur Research Solutions | This paper doesn‚Äôt seem particularly useful. However, datasets like V's A and B on human behavior monitoring appear interesting. It may be worthwhile to collect daily behavior patterns across different industries similarly. The National Household Travel Survey Dataset and the Activity-Based Model Dataset from the Southern California Association of Governments are also marked for further exploration. |
| MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models | NVIDIA, National University of Singapore | This paper is recommended for reading, as it introduces a novel approach using learnable masks to achieve domain adaptation. It hints at an important realization: rare downstream patterns learned during pretraining may be learned correctly, but not properly aligned. The method is innovative. |
| Exploring Semantic Clustering in Deep Reinforcement Learning for Video Games | University of Arizona | This paper investigates which video games are similar to each other in terms of gameplay. |
| Post-hoc Reward Calibration: A Case Study on Length Bias | University of Edinburgh, Alibaba Group | This paper is recommended for reading. It deals with reward post-editing in an intuitive way: estimating bias first, then removing that bias from the reward. The approach feels very adaptable. |
| Search for Efficient Large Language Models | Northeastern University, Harvard University, Oracle | This title is marked for future exploration. |
| FreeEdit: Mask-free Reference-based Image Editing with Multi-modal Instruction | Institute of Information Engineering (CAS) | This title is also marked for future exploration. |
| HydraViT: Stacking Heads for a Scalable ViT | Kiel University | Another title marked for further evaluation. |
| Why Companies "Democratize" Artificial Intelligence: The Case of Open Source Software Donations |  | This paper systematically analyzes the benefits of companies sponsoring open-source projects. The conclusion is that it‚Äôs highly beneficial, suggesting more open-source contributions to save money, gain visibility, and boost morale. |
| Inference-Time Language Model Alignment via Integrated Value Guidance | Shanghai Artificial Intelligence Laboratory, Shanghai Jiaotong University | This paper explores alignment or enhanced alignment during inference. If instruction following is a mechanical pattern for LLMs, it may be possible to achieve training-free alignment by controlling the attention pattern composition. Stanford had a related work recently. |
| Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models | CISPA Helmholtz Center for Information Security | This paper is recommended for reading. It claims to establish LLMs as the state-of-the-art for using priors in gradient compression. |
| Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness | Tencent, Jilin University | Another recommended read. The idea is simple yet effective: human preferences are not binary but exist on a continuum, and this should be reflected in modeling. The method itself is relatively unimportant, but the intuition is solid. |
| What Would You Ask When You First Saw a¬≤ + b¬≤ = c¬≤? Evaluating LLM on Curiosity-Driven Questioning | Stevens Institute of Technology | This paper explores the interesting question of how well LLMs can actively acquire knowledge to fill in gaps. |
| HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and Dynamic Workflows | Tencent | This title is also marked for future evaluation. |
| CSCE: Boosting LLM Reasoning by Simultaneously Enhancing Causal Significance and Consistency | University of Science and Technology Beijing | This paper is highly recommended. The author agrees with the notion that after each inference step, the most important thing to observe is whether the current step contributes to solving the main problem. The optimization shouldn‚Äôt be for a chain but for dependencies‚Äîsolving subproblems that contribute to the main issue. However, the method is somewhat crude. |

</table>

</details>


<details>
<summary> <b>26/9/2024</b> </summary>

| Paper  | Affiliation | Comments |
|:-------------|:-------------|:-------------|
| Beyond Following: Mixing Active Initiative into Computational Creativity | Georgia Institute of Technology | This is an HCI study on User-AI collaborative creation. There were many similar studies a year or two ago, and it seems to be an unexplored pattern of creativity, beyond just instruction following. |
| HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale | FPT Software AI Center | This is a multi-agent software engineering (SE) framework. The introduction of features like Feature Localization and Edition distinguishes it from previous frameworks and aligns more with agile development workflows. |
| Task-oriented Prompt Enhancement via Script Generation | York University | SoT + PoT‚Äîthese types of XoT frameworks, after o1, show that their most obvious value lies in how to mine scalable, high-confidence, similar datasets to train models. |
| VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models | Microsoft, USTC | No further comments were provided, but it seems the title is flagged for its relevance to post-training quantization techniques. |
| Harnessing Diversity for Important Data Selection in Pretraining Large Language Models | Beijing Institute of Technology, SenseTime, Shanghai AI Lab | This presents a pretraining data quality estimation framework based on clustering and downstream performance. While it doesn‚Äôt feel very useful for pretraining itself, it could hold value for selecting pretraining data sources for supervised fine-tuning (SFT). |
| Algorithmic Drift: A Simulation Framework to Study the Effects of Recommender Systems on User Preferences | ICAR-CNR | No specific feedback was provided, but it appears to be flagged for exploring the impact of algorithms on user behavior. |
| GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for Improved Visual Localization | BE2R Lab, ITMO University | This title seems to be acknowledged for its innovative approach to improving visual localization through 3D Gaussian splatting. |
| Demystifying Issues, Causes, and Solutions in LLM Open-Source Projects | Wuhan University, RMIT University | This was described as an interesting read, analyzing issues in LLM open-source projects. |
| Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification | The University of Tokyo | The user pointed to Figure 5, which shows the actual impact on distribution. The user agrees that CLIP has many unnatural aspects‚Äîfor instance, humans determine what to focus on based on their own context, and this is reflected in many benchmarks. The bottleneck in MLLM's image encoder pretraining is severe, offering significant room for improvement. |
| Uncertainty Representations in State-Space Layers for Deep Reinforcement Learning under Partial Observability | Bosch Corporate Research | - |
| Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question Answering | University of Technology Sydney | The methods in this paper are highly questionable and can be ignored. However, the four TSQA benchmarks used might have some value, especially for LLM users encountering corner cases. |
| Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI | University of T√ºbingen | - |
| Counterfactual Token Generation in Large Language Models | Max Planck Institute for Software Systems | - |
| INT-FlashAttention: Enabling Flash Attention for INT8 Quantization | Peking University, Baichuan Inc, Beihang University | This describes Baichuan‚Äôs solution combining INT8 and Flash Attention. |
| How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not | La Sapienza, University of Rome | - |
| Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale | Shanghai Jiao Tong University, Shanghai AI Lab, Sea AI Lab | This paper presents an LLM-based pretrain data filtering method, releasing related datasets. The user believes this is the future direction and recommends reading it. |
| FineZip: Pushing the Limits of Large Language Models for Practical Lossless Text Compression | UC Berkeley | - |
| AXCEL: Automated eXplainable Consistency Evaluation using LLMs | Amazon | - |
| Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents | Peking University, Microsoft | The paper discusses creating many useful agents by combining short-chain CoT (Chain-of-Thought) with tool usage rather than long-chain CoT (considering possible redundancy). This agent work is interesting, and the formation of APIs can serve as reasoning shortcuts. The process is similar to writing code and encapsulating functions, making it worth recommending. |
| Tell Me What You Don't Know: Enhancing Refusal Capabilities of Role-Playing Agents via Representation Space Analysis and Editing | Fudan University, YouTu Lab, Tencent | This paper examines enhancing refusal capabilities in role-playing agents, flagged as potentially interesting. |
| Multi-objective Evolution of Heuristic Using Large Language Model | CUHK | - |
| Attention Prompting on Image for Large Vision-Language Models | NUS | This is recommended for reading. It introduces a naive but effective trick‚Äîhumans tend to focus on different parts of an image depending on context. A promising research direction would be how to adjust CLIP embeddings at inference time based on context. |
| A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms | Beihang University, ETH Zurich, SenseTime | This is flagged for reading, covering low-bit language models. |
| Dynamic-Width Speculative Beam Decoding for Efficient LLM Inference | UCLA | This paper presents a dynamic-width decoding method aimed at making LLM inference more efficient. |
| Unsupervised Text Representation Learning via Instruction-Tuning for Zero-Shot Dense Retrieval | Penn State University, Amazon AGI | - |

</details>

<details>
<summary> <b>25/9/2024</b> </summary>

| Paper  | Affiliation | Comments |
|:-------------|:-------------|:-------------|
| HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models | Beihang University, Shanghai AI Lab, M-A-P | This project was done with interns and is currently the best open-source benchmark for long text generation available in the market. The Open-Ended QA and Heuristic Text Generation sections are particularly valuable. Hello, HelloBench! |
| MonoFormer: One Transformer for Both Diffusion and Autoregression | Baidu VIS, University of Technology Sydney | This combines diffusion and autoregressive methods in a straightforward manner. |
| EuroLLM: Multilingual Language Models for Europe | Unbabel, Instituto Superior T√©cnico | Strongly recommended for reading! The Joint Scaling Law is interesting and may be extrapolated to domain-specific ratios. Once the number of domains increases, it could provide a higher return on investment compared to the D-CPT Law and similar optimization functions. |
| MaskBit: Embedding-free Image Generation via Bit Tokens | ByteDance, Technical University Munich, CMU | - |
| CJEval: A Benchmark for Assessing Large Language Models Using Chinese Junior High School Exam Data | Tencent YouTu Lab, Peking University | Recommended for reading; it is a high-quality benchmark. Notably, they mentioned that Tencent annotated over 20,000 exam questions for CoT and the cost involved; it might be better to use that for training. |
| LLM Echo Chamber: Personalized and Automated Disinformation | Imperial College London | - |
| On the Complexity of Neural Computation in Superposition | MIT | - |
| Watch Your Steps: Observable and Modular Chains of Thought | CMU | This is a CMU version of DoT. It seems quite similar to a recent paper from THU but is written with more detail and clarity. Mechanically, there are some differences, but the way it identifies and names steps; defines the input/output behavior of steps; and replaces CoT explanations with chains of these formalized steps on the same examples is clearer. |
| Reward-Robust RLHF in LLMs | THU, Institute of Automation CAS, Baichuan AI | - |
| Smirk: An Atomically Complete Tokenizer for Molecular Foundation Models | Umich Ann Arbor, CMU | - |
| ControlMath: Controllable Data Generation Promotes Math Generalist Models | Hong Kong University of Science and Technology (Guangzhou), Hong Kong University of Science and Technology | This approach retains only the more challenging questions during each round of synthetic data iteration to enhance the difficulty of the mathematical problems generated by the model. |
| Parse Trees Guided LLM Prompt Compression | Tsinghua University, Fuyao University of Science and Technology | - |
| Tag Map: A Text-Based Map for Spatial Reasoning and Navigation with Large Language Models | ETH Zurich | - |
| In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models | Carleton College, California Institute of Technology | This raises an important issue: LLMs are not sensitive to trivial context changes, leading to the A-Not-B problem. |
| VLMine: Long-Tail Data Mining with Vision Language Models | Cruise LLC, Meta Inc. | - |
| TFG: Unified Training-Free Guidance for Diffusion Models | Stanford University, Peking University, Tsinghua University | - |
| Empirical Insights on Fine-Tuning Large Language Models for Question-Answering | Fudan University, Lenovo Research | Findings 1 and Figure 4 are quite interesting. |
| Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection | King‚Äôs College London | This is an interesting read with a quite engaging automatic hyperparameter search mechanism. |
| Fine-Tuning is Fine, if Calibrated | The Ohio State University, University of Wisconsin Madison | - |
| Steward: Natural Language Web Automation | University of Michigan | - |
| CLSP: High-Fidelity Contrastive Language-State Pre-training for Agent State Representation | Shanghai AI Laboratory | - |

</details>

<details>
<summary> <b>24/9/2024</b> </summary>

| Paper  | Affiliation | Comments |
|:-------------|:-------------|:-------------|
| OmniBench: Towards The Future of Universal Omni-Language Models | M-A-P, UoM | The author put significant effort into annotating their paper. Each data point in OmniBench requires simultaneous use of audio, image, and text information to provide answers. Every data point is a carefully checked sample by co-authors. Lacking information from one modality results in guessing from 2-3 options. Two important takeaways are: 1. Currently, all open-source models claiming to be Omni models do not possess the ability to process information from three modalities simultaneously. Closed-source models have limited capabilities and have not achieved cross-modal generalization. 2. The performance of MLLM is constrained under the conditions of image description, audio transcription, and text response, with Gemini nearing this upper limit, and Reka-core performing around 10 points below it. However, information loss remains significant for this part of the encoder. It is hoped that GDM will have a similar approach internally, potentially becoming a necessary benchmark for future OLMs. |
| Can-Do! A Dataset and Neuro-Symbolic Grounded Framework for Embodied Planning with Large Multimodal Models | Singapore University of Technology and Design, DAMO Academy | A potentially useful embodied multimodal benchmark. |
| LLMs are One-Shot URL Classifiers and Explainers | University of Sydney, University of New South Wales | Six months ago, a similar observation was made. Our experiments show that both 3.5 and Claude are very effective URL classifiers, capable of determining the source of a webpage based on its root domain. The significant value of this property lies in its ability to collect a large amount of domain seed data, which can be used to cold start training FastText and BERT. This can further aid in classifying already collected pretraining data. We plan to release a pre-training dataset with 67 domain categories in 2-3 weeks. |
| A-VL: Adaptive Attention for Large Vision-Language Models | University of Science and Technology of China, NIO Inc | In MLLMs, images and text exhibit different self-attention patterns. Therefore, two cache modes are designed for inference, one for the visual part that stores potentially useful information while computing only the most critical parts, and another for the language part that emphasizes local information. |
| VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models | The Chinese University of Hong Kong, The Hong Kong University of Science and Technology (Guangzhou) | A benchmark for the generalizability of text-to-image models. |
| Distribution-Level Feature Distancing for Machine Unlearning: Towards a Better Trade-off Between Model Utility and Forgetting | Yonsei University, MODULABS, POSTECH | The introduction of Optimal Transport helps avoid misguiding the model during forgetting. Personally, I have read few papers on this topic, but this approach seems quite reasonable. |
| Target-Aware Language Modeling via Granular Data Sampling | AI at Meta, Virginia Tech, Iowa State University | By introducing n-gram features related to downstream tasks, relevant data can be identified to enhance the performance of downstream tasks without compromising other task performance. |
| Past Meets Present: Creating Historical Analogy with Large Language Models | Fudan University, ByteDance Inc. | Congratulations! |
| Do Large Language Models have Problem-Solving Capability under Incomplete Information Scenarios? | Fudan University, SenseAuto, Carnegie Mellon University | The design concept of a good interactive benchmark is commendable, but the data quality and differentiation appear to be lacking, and the hard mode does not seem too challenging. Examples include "Who is Undercover" and "Twenty Questions." |
| VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models | POSTECH | 1. VLMs show varying sensitivity to different colors but consistently exhibit insensitivity to green across various VLMs. 2. There are differences in shape sensitivity and semantic recognition depending on the LLM's capacity, even with the same fixed visual encoder. |
| Identify As A Human Does: A Pathfinder of Next-Generation Anti-Cheat Framework for First-Person Shooter Games | University of Hong Kong, Stevens Institute of Technology | A framework for anti-cheat mechanisms in games. |
| Orthogonal Finetuning for Direct Preference Optimization | Institute of Information Engineering (CAS), School of Cyber Security (CAS) | - |
| Inference-Friendly Models With MixAttention | Databricks Mosaic AI | - |
| Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task | Lingua Custodia | - |
| Efficiently Dispatching Flash Attention For Partially Filled Attention Masks | University of T√ºbingen, T√ºbingen AI Center | - |
| Style over Substance: Failure Modes of LLM Judges in Alignment Benchmarking | Arthur AI, NYU, Columbia University | Analysis of judgment bias in LLM-based evaluations. |
| A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor? | UC Santa Cruz, University of Edinburgh, National Institutes of Health | A rapid review of O1 in medicine; the conclusion suggests that there has not been significant improvement. |
| VisScience: An Extensive Benchmark for Evaluating K12 Educational Multi-modal Scientific Reasoning | Tsinghua University, Beihang University, Zhipu.AI | Similar to MMMU Bench, this may be useful. |
| Drift to Remember | University of Minnesota Twin Cities, Harvard University | - |
| You Only Use Reactive Attention Slice For Long Context Retrieval | University of California | Utilizing a circuit-based approach for RAG, a Meta project, seems intuitive and makes sense. Recommended reading. |
| Prompt Baking | Caltech, University of Toronto | A lightweight, prompt-based model parameter personalization scheme. Recommended reading; it is an interesting trick. |
| RNR: Teaching Large Language Models to Follow Roles and Rules | Georgia Institute of Technology, Amazon | A potentially useful role-playing benchmark. |
| Language agents achieve superhuman synthesis of scientific knowledge | FutureHouse Inc., University of Rochester | A useful benchmark. |
| MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model | Tsinghua University, Beihang University, Zhipu.AI | - |
| A Multi-LLM Debiasing Framework | Stanford University, Adobe Research | - |
| One-shot World Models Using a Transformer Trained on a Synthetic Prior | University of Freiburg, ELLIS Institute T√ºbingen | - |
| ChemEval: A Comprehensive Multi-Level Chemical Evaluation for Large Language Models | University of Science and Technology of China, State Key Laboratory of Cognitive Intelligence | A useful specialized benchmark. |
| A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders | LASR Labs, University College London, Decode Research | Recommended reading; describes a phenomenon in Sparse Autoencoders known as feature absorption, which may affect their reliability. |
| Proof Automation with Large Language Models | Purdue University | Recommended reading; this work also decouples low-level proofs from high-level speculative decoding in formal verification. |
| TracrBench: Generating Interpretability Testbeds with Large Language Models | University of Bern | A high-quality benchmark. |

</details>

<hr/>

If you are intereted in the work published by us, please navigate to our [full paper list](https://huggingface.co/collections/m-a-p/m-a-p-full-paper-list-65e070a694c7b01c5547fbff).
