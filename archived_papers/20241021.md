<h1 align="center"><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg" width="25"> M-A-P Daily Paper</h1>
<p align="center">
<a href="https://github.com/DenverCoder1/readme-typing-svg"><img src="https://media.giphy.com/media/Rn26lWjqA0uUU/giphy.gif" width="100"></a>
</p>
<hr/>
<h4 align="center">The <a href=https://m-a-p.ai>M-A-P</a> daily paper project curates and reviews a selection of new papers published daily on arXiv, providing insightful commentary on cutting-edge research across various scientific disciplines.</h4>
<br>
<hr/>

[back to main page](https://m-a-p.ai/DailyPaper)


## üõ†Ô∏è Papers This Week 

(Expand to View)

<details>
<summary> <b>21/10/2024</b> </summary>

<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| [Do LLMs "know" internally when they follow instructions?](https://arxiv.org/pdf/2410.14516) | The study employs linear probes across different layers (early/middle/last layers) and different positions of tokens (first/middle/last token) to identify whether modifying representations along with dimension in the input embedding space links to successful instruction-following. This methodology connects with another recent relevant work 'Improving Instruction-Following in Language Models through Activation Steering.' From the perspective of mechanical interpretability, the findings demonstrate the capability of linear probing in identifying the parameters in even an abstract scenario like instruction-following. This can be effectively generalised to identifying patterns in CoT. It can also be utilised in activating more effective reasoning patterns through activation steering. This is a promising research direction. The value of parameter probing this kind of methodology appears underappreciated in the field. |
| [Do LLMs estimate uncertainty well in instruction-following?](https://arxiv.org/pdf/2410.14582) | The methodology for cross-model uncertainty comparison in this paper requires further verification. Some of the propsoed methods are based on probability and mean token entropy. The study identifies normalized p(true) as the most reliable evaluation metric. Additional verification is needed to understand its cross-model applicability of these metrics. The evolution of uncertainty during pre-training merits further investigation. |
| [MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts](https://arxiv.org/pdf/2410.14574) | The introduction of momentum into SMoE raises negative effects to computational efficiency and the bound of model architectureÔºåregarding Formula 9 in the paper. The paper lacks clear justification for the crutial meaning of dynamics of the expert representations in SMoEs. |
| [Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection](https://arxiv.org/pdf/2410.14581) | The paper presents a novel attention mechanism. |
| How Does Data Diversity Shape the Weight Landscape of Neural Networks? | Key findings include: 1) Dropout tends to promote more uniform distribution of empirical spectral density (ESD), while weight decay leads to heavier tails. 2) Data diversity's effects on weight matrices align with dropout's impact and contrast with weight decay's effects. |
| [Streaming Deep Reinforcement Learning Finally Works](https://arxiv.org/pdf/2410.14606) | This paper proposes a method to stabilize Streaming DRL. |
| Supervised Chain of Thought | The paper's primary contribution lies in introducing the concept of prompt space complexity. It proposes that search complexity is determined by total information volume and per-step information extraction, defined as C(m,s). This framework offers a more well-defined approach to quantifying CoT requirements across different task types compared to the vaguer concept of hops. |
| Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction | Notable for its design motivation and abstraction of linear subregions with self-convergent symbolic linear representations. The implications for CoT attention mechanisms suggest natural language generation may operate through state transitions corresponding to subregion representations. Questions remain regarding the symbolic linearity of these representations. |
| RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training | Addresses the widely acknowledged information bottleneck issue in MLLM encoders. The approach targets specific token-patch correspondences. Potential improvements could involve dynamic, context-aware sub-image framing based on text embeddings, though training complexity may present challenges. |
| Speciesism in Natural Language Processing Research | Documents LLMs' learned biases regarding non-human animals, reflecting human prejudices. |
| Associative memory and dead neurons | Examines neurons exhibiting activation function saturation. Merits further investigation. |
| Latent Weight Diffusion: Generating Policies from Trajectories | Presents potential benefits for cross-game Decision Transformer generalization. The approach models different policy behaviors using latent variable z, deriving target policy function distributions through conditional independence. The policy representation shows promise for cross-game generalization. |
| On Partial Prototype Collapse in the DINO Family of Self-Supervised Methods | Further analysis pending. |
| Provable Benefits of Complex Parameterizations for Structured State Space Models | Provides experimental validation of complex parameterization benefits for SSMs. Key finding demonstrates higher dimensional utilization in complex parameterization, though experiments remain relatively simple. Formula verification pending. |
| In-context learning and Occam's razor | Noteworthy sections include 3.1 and 3.5, highlighting prefix encoding differences and theoretically establishing prefix encoding length as a tight upper bound for dataset and model complexity. The framework redefines ICL-based meta-learning objectives through minimizing prefix encoding length across multiple tasks. |
| RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph | While not groundbreaking, the graph-based code modeling approach aligns with effective representation strategies for both code and mathematics. Connected to early attempts at RL-optimized reasoning for DAGs. |

</table>

</details>
<hr/>

If you are intereted in the work published by us, please navigate to our [full paper list](https://huggingface.co/collections/m-a-p/m-a-p-full-paper-list-65e070a694c7b01c5547fbff).
