<h1 align="center"><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg" width="25"> M-A-P Daily Paper</h1>
<p align="center">
<a href="https://github.com/DenverCoder1/readme-typing-svg"><img src="https://media.giphy.com/media/Rn26lWjqA0uUU/giphy.gif" width="100"></a>
</p>
<hr/>
<h4 align="center">The <a href=https://m-a-p.ai>M-A-P</a> daily paper project curates and reviews a selection of new papers published daily on arXiv, providing insightful commentary on cutting-edge research across various scientific disciplines.</h4>
<br>
<hr/>

[back to main page](https://m-a-p.ai/DailyPaper)


## üõ†Ô∏è Papers This Week 

(Expand to View)

<details>
<summary> <b>21/10/2024</b> </summary>

<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| [Do LLMs "know" internally when they follow instructions?](https://arxiv.org/pdf/2410.14516) | The study employs linear probes across different layers (early/middle/last layers) and different positions of tokens (first/middle/last token) to identify whether modifying representations along with dimension in the input embedding space links to successful instruction-following. This methodology connects with another recent relevant work 'Improving Instruction-Following in Language Models through Activation Steering.' From the perspective of mechanical interpretability, the findings demonstrate the capability of linear probing in identifying the parameters in even an abstract scenario like instruction-following. This can be effectively generalised to identifying patterns in CoT. It can also be utilised in activating more effective reasoning patterns through activation steering. This is a promising research direction. The value of parameter probing this kind of methodology appears underappreciated in the field. |
| [Do LLMs estimate uncertainty well in instruction-following?](https://arxiv.org/pdf/2410.14582) | The methodology for cross-model uncertainty comparison in this paper requires further verification. Some of the propsoed methods are based on probability and mean token entropy. The study identifies normalized p(true) as the most reliable evaluation metric. Additional verification is needed to understand its cross-model applicability of these metrics. The evolution of uncertainty during pre-training merits further investigation. |
| [MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts](https://arxiv.org/pdf/2410.14574) | The introduction of momentum into SMoE raises negative effects to computational efficiency and the bound of model architectureÔºåregarding Formula 9 in the paper. The paper lacks clear justification for the crutial meaning of dynamics of the expert representations in SMoEs. |
| [Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection](https://arxiv.org/pdf/2410.14581) | The paper presents a novel attention mechanism. |
| [How Does Data Diversity Shape the Weight Landscape of Neural Networks?](https://arxiv.org/pdf/2410.14602) | Key findings include: 1) Dropout tends to promote more uniform distribution of empirical spectral density (ESD), while weight decay leads to heavier tails. 2) The impact of data diversity on weight matrices aligns with the effect of dropout but contrasts with that of weight decay. |
| [Streaming Deep Reinforcement Learning Finally Works](https://arxiv.org/pdf/2410.14606) | This paper proposes a method to stabilize Streaming DRL. |
| [Supervised Chain of Thought](https://arxiv.org/pdf/2410.14198) | The paper's primary contribution lies in introducing the concept of prompt search complexity. It proposes that search complexity depends on both total information in latent vector and amount of information each CoT step can extract, defined as C(m,s). This framework offers a more well-defined approach to quantifying CoT requirements across different task types compared to the vaguer concept hops as the amount of information is more quantifiable. |
| [Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction](https://arxiv.org/pdf/2410.14240) | Recommended reading. The motivation proposed in this work is notable for its abstraction of linear subregions and the most parsimonious representation of linear subregions. This framework appears natural for understanding the existence of attention in Chain of Thought (CoT) processes. If we consider language generation not as a word-by-word process, but rather as a switching state system where content is planned and then expressed, then switch to the next state. These transitions between states might correspond to representations of certain subregions. But are these symbolically linear. A pertinent question arose regarding whether neural architecture should directly emulate human brain if Neural Text Processing (NTP) and Supervised Fine-Tuning (SFT) are forms of imitation learning. Human primitive interaction patterns fundamentally align more closely with feedback-based learning mechanisms, essentially representing a scaled implementation of reinforcement learning (RL). From this theoretical perspective, even the Neural Text Processing (NTP) paradigm can be considered an ad-hoc solution. While the current developmental stage necessitates the incorporation of imitation learning for fundamental pattern acquisition, it suggests a potential evolutionary trajectory for NTP: transitioning from word-level processing to higher-order, more dynamic level. This hypothesis is supported by the inherent existence of hierarchical transitional logic structures in language output and composition, independent of neurological architecture. The manifestation of these patterns in language generation persists whether the objective is to emulate neural processes or natural human linguistic output patterns. |
| [RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training](https://arxiv.org/pdf/2410.14154) | (!) This work addresses the widely acknowledged information bottleneck issue in MLLM encoders. The approach targets specific token-patch correspondences. Potential improvements could involve dynamic, context-aware sub-image framing based on text embeddings, though training complexity may present challenges. |
| [Speciesism in Natural Language Processing Research](https://arxiv.org/pdf/2410.14194#:~:text=First%2C%20if%20there%20is%20a,speciesism%20bias%20in%20NLP%20models.) | An interesting finding of this work is recent LLMs exhibit speciesist bias. |
| [Associative memory and dead neurons](https://arxiv.org/pdf/2410.13866) | (!) This work examines neurons exhibiting activation function saturation. It might be valuable. |
| Latent Weight Diffusion: Generating Policies from Trajectories | Presents potential benefits for the generalization of cross-game Decision Transformer. The approach models different policy behaviors using latent variable z, deriving target policy function distributions through conditional independence. The policy representation shows promise for cross-game generalization. |
| [On Partial Prototype Collapse in the DINO Family of Self-Supervised Methods](https://arxiv.org/pdf/2410.14060) | - |
| [Provable Benefits of Complex Parameterizations for Structured State Space Models(https://arxiv.org/pdf/2410.14067) | This work empirically demonstrate the benefits of complex parameterizations for SSMs. Key finding demonstrates more efficient utilization of dimention in complex SSMs, though experiments remain relatively simple. |
| [In-context learning and Occam's razor](https://arxiv.org/pdf/2410.14086) | Highly recommended reading. Noteworthy sections include 3.1 and 3.5, analysing the influence of prequential coding. The length of prequential coding, as the upper-bound of the data and model, is tight. It's very insightful that it shows how learning algorithms can be used to compress data through prequential coding, and that minimizing the resulting ‚Äúprequential code length‚Äù achieved by a learning algorithm is equivalent to jointly minimizing the training error and complexity of the model it fits. The commenter was thinking of why data mixture is effective. The thought is its efficacy stems not from reweighting mechanisms, but rather from partial ordering and pre-training dynamics. Further hypothesis: the fundamental value of mixture approaches lies in their capacity to enhance the probability of correctly learning partial ordering. This necessitates developing a framework for attributing dependencies among different samples. To illustrate this concept, consider the acquisition of university-level knowledge without proper exposure to dependent the knowledge learned in secondary school. In such scenarios, three potential outcomes emerge: Just rote memorised the knowledge in university; learnt only noise; learnt non-robust, unstable knowledge. This analysis suggests that data scheduler design may be crucial for future pre-training methodologies, particularly in modeling correct partial ordering. |
| [RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph](https://arxiv.org/pdf/2410.14684) | This work is not interesting. But the commenter agrees modeling code or mathematics algorithms using graph. Here is an another early attempt called Steiner, a series of reasoning models trained on synthetic data using RL, constructing a DAG in its model. |

</table>

</details>
<hr/>

If you are intereted in the work published by us, please navigate to our [full paper list](https://huggingface.co/collections/m-a-p/m-a-p-full-paper-list-65e070a694c7b01c5547fbff).
