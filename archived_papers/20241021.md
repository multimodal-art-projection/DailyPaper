<h1 align="center"><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg" width="25"> M-A-P Daily Paper</h1>
<p align="center">
<a href="https://github.com/DenverCoder1/readme-typing-svg"><img src="https://media.giphy.com/media/Rn26lWjqA0uUU/giphy.gif" width="100"></a>
</p>
<hr/>
<h4 align="center">The <a href=https://m-a-p.ai>M-A-P</a> daily paper project curates and reviews a selection of new papers published daily on arXiv, providing insightful commentary on cutting-edge research across various scientific disciplines.</h4>
<br>
<hr/>

[back to main page](https://m-a-p.ai/DailyPaper)


## üõ†Ô∏è Papers This Week 

(Expand to View)

<details>
<summary> <b>21/10/2024</b> </summary>

<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| [Do LLMs "know" internally when they follow instructions?](https://arxiv.org/pdf/2410.14516) | The study employs linear probes across different layers (early/middle/last layers) and different positions of tokens (first/middle/last token) to identify whether modifying representations along with dimension in the input embedding space links to successful instruction-following. This methodology connects with another recent relevant work 'Improving Instruction-Following in Language Models through Activation Steering.' From the perspective of mechanical interpretability, the findings demonstrate the capability of linear probing in identifying the parameters in even an abstract scenario like instruction-following. This can be effectively generalised to identifying patterns in CoT. It can also be utilised in activating more effective reasoning patterns through activation steering. This is a promising research direction. The value of parameter probing this kind of methodology appears underappreciated in the field. |
| [Do LLMs estimate uncertainty well in instruction-following?](https://arxiv.org/pdf/2410.14582) | The methodology for cross-model uncertainty comparison in this paper requires further verification. Some of the propsoed methods are based on probability and mean token entropy. The study identifies normalized p(true) as the most reliable evaluation metric. Additional verification is needed to understand its cross-model applicability of these metrics. The evolution of uncertainty during pre-training merits further investigation. |
| [MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts](https://arxiv.org/pdf/2410.14574) | The introduction of momentum into SMoE raises negative effects to computational efficiency and the bound of model architectureÔºåregarding Formula 9 in the paper. The paper lacks clear justification for the crutial meaning of dynamics of the expert representations in SMoEs. |
| [Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection](https://arxiv.org/pdf/2410.14581) | The paper presents a novel attention mechanism. |
| [How Does Data Diversity Shape the Weight Landscape of Neural Networks?](https://arxiv.org/pdf/2410.14602) | Key findings include: 1) Dropout tends to promote more uniform distribution of empirical spectral density (ESD), while weight decay leads to heavier tails. 2) The impact of data diversity on weight matrices aligns with the effect of dropout but contrasts with that of weight decay. |
| [Streaming Deep Reinforcement Learning Finally Works](https://arxiv.org/pdf/2410.14606) | This paper proposes a method to stabilize Streaming DRL. |
| [Supervised Chain of Thought](https://arxiv.org/pdf/2410.14198) | The paper's primary contribution lies in introducing the concept of prompt search complexity. It proposes that search complexity depends on both total information in latent vector and amount of information each CoT step can extract, defined as C(m,s). This framework offers a more well-defined approach to quantifying CoT requirements across different task types compared to the vaguer concept hops as the amount of information is more quantifiable. |
| [Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction](https://arxiv.org/pdf/2410.14240) | Recommended reading. The motivation proposed in this work is notable for its abstraction of linear subregions and the most parsimonious representation of linear subregions. This framework appears natural for understanding the existence of attention in Chain of Thought (CoT) processes. If we consider language generation not as a word-by-word process, but rather as a switching state system where content is planned and then expressed, then switch to the next state. These transitions between states might correspond to representations of certain subregions. But are these symbolically linear. A pertinent question arose regarding whether neural architecture should directly emulate human brain if Neural Text Processing (NTP) and Supervised Fine-Tuning (SFT) are forms of imitation learning. Human primitive interaction patterns fundamentally align more closely with feedback-based learning mechanisms, essentially representing a scaled implementation of reinforcement learning (RL). From this theoretical perspective, even the Neural Text Processing (NTP) paradigm can be considered an ad-hoc solution.
While the current developmental stage necessitates the incorporation of imitation learning for fundamental pattern acquisition, it suggests a potential evolutionary trajectory for NTP: transitioning from word-level processing to higher-order, more dynamic level. This hypothesis is supported by the inherent existence of hierarchical transitional logic structures in language output and composition, independent of neurological architecture. The manifestation of these patterns in language generation persists whether the objective is to emulate neural processes or natural human linguistic output patterns. |
| RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training | Addresses the widely acknowledged information bottleneck issue in MLLM encoders. The approach targets specific token-patch correspondences. Potential improvements could involve dynamic, context-aware sub-image framing based on text embeddings, though training complexity may present challenges. |
| Speciesism in Natural Language Processing Research | Documents LLMs' learned biases regarding non-human animals, reflecting human prejudices. |
| Associative memory and dead neurons | Examines neurons exhibiting activation function saturation. Merits further investigation. |
| Latent Weight Diffusion: Generating Policies from Trajectories | Presents potential benefits for cross-game Decision Transformer generalization. The approach models different policy behaviors using latent variable z, deriving target policy function distributions through conditional independence. The policy representation shows promise for cross-game generalization. |
| On Partial Prototype Collapse in the DINO Family of Self-Supervised Methods | Further analysis pending. |
| Provable Benefits of Complex Parameterizations for Structured State Space Models | Provides experimental validation of complex parameterization benefits for SSMs. Key finding demonstrates higher dimensional utilization in complex parameterization, though experiments remain relatively simple. Formula verification pending. |
| In-context learning and Occam's razor | Noteworthy sections include 3.1 and 3.5, highlighting prefix encoding differences and theoretically establishing prefix encoding length as a tight upper bound for dataset and model complexity. The framework redefines ICL-based meta-learning objectives through minimizing prefix encoding length across multiple tasks. |
| RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph | While not groundbreaking, the graph-based code modeling approach aligns with effective representation strategies for both code and mathematics. Connected to early attempts at RL-optimized reasoning for DAGs. |

</table>

</details>
<hr/>

If you are intereted in the work published by us, please navigate to our [full paper list](https://huggingface.co/collections/m-a-p/m-a-p-full-paper-list-65e070a694c7b01c5547fbff).
