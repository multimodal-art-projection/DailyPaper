<h1 align="center"><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg" width="25"> M-A-P Daily Paper</h1>
<p align="center">
<a href="https://github.com/DenverCoder1/readme-typing-svg"><img src="https://media.giphy.com/media/Rn26lWjqA0uUU/giphy.gif" width="100"></a>
</p>
<hr/>
<h4 align="center">The <a href=https://m-a-p.ai>M-A-P</a> daily paper project curates and reviews a selection of new papers published daily on arXiv, providing insightful commentary on cutting-edge research across various scientific disciplines.</h4>
<br>
<hr/>

[back to main page](https://m-a-p.ai/DailyPaper)


## üõ†Ô∏è Papers This Week 

(Expand to View)

<details>
<summary> <b>21/10/2024</b> </summary>

<table class="center">

| Paper | Comments |
|:-------------|:-------------|
| SMART: Self-learning Meta-strategy Agent for Reasoning Tasks | The environment definition and learning objectives present noteworthy aspects, particularly the goal of enabling LMs to learn and select optimal strategies on first attempts. The process is modeled as an MDP using reinforcement learning. While the environmental setup and learning process are well-conceived, the paper appears preliminary in nature. The thought classification system is relatively basic with only three categories, and the investigation of thought hierarchy, especially fine-grained thought definitions, remains unexplored. Shows potential for further development. |
| Improve Vision Language Model Chain-of-thought Reasoning | Demonstrates significant improvements in various VQA benchmarks through CoT supervised training of MLLM. Limited additional insights provided. Pipeline and data generation methods await release. |
| Reflection-Bench: probing AI intelligence with reflection | Presents a candidate OOD benchmark, designing seven LLM evaluation tasks: Perception, Memory, Decision-Making, Prediction, Belief Updating, Counterfactual Thinking, and Meta Reflection. |
| Are Language Model Logits Calibrated? | Introduces interesting calibration definitions using Wasserstein distance, with potential for extension. Defines calibration as the consistency between output probabilities of candidate words and their relative likelihood inferred from context. Key finding: instruction-tuned models demonstrate poor calibration and relative entropy performance, with notable pattern collapse. This may represent a significant concern for current alignment methodologies. |
| InternLM2.5-StepProver | Shanghai AI Lab's Lean model employs best-first search and critic-guided search for proof generation. Initial phase utilizes InternLM2-StepProver for rapid scanning, incorporating discovered proofs into the training set while removing solved problems and their negations. Implements robust speculative decoding optimizations and Critic Model updates. The quantitative resource assessment analysis suggests that correct proof generation paths and introduced mathematical tools result in shorter paths compared to erroneous routes. Despite recent focus shifts, Lean development remains valuable for generating accurate, lengthy CoT instances. |
| How to Build a Pre-trained Multimodal model for Simultaneously Chatting and Decision-making? | Presents a valuable and natural problem definition: an MLLM-capable agent that processes visual input and generates both conversational responses and action predictions. Effectively combines standard MLLM approaches with Genie, though methodologically conventional. The framework suggests potential for a new multimodal model category incorporating embodied actions with optional verbal interaction. |
| Chasing Random: Instruction Selection Strategies Fail to Generalize | Concludes instruction selection strategies and metrics have limited utility. Analysis appears incomplete due to limitations in data pools (FLAN, Dolly) and insufficient consideration of data distribution. The research direction maintains value as instruction data pools expand, suggesting focus should shift from quality metrics to distribution analysis. |
| Long Term Memory: The Foundation of AI Self-Evolution | Presents valuable thought experiments and system design concepts. Two significant insights emerge: 1) The importance of cognitive accumulation, though its definition throughout pre-training may be questionable. 2) The crucial transition from imitation learning to learning from feedback, highlighting current RLHF limitations in terms of cost and methodology. |
| Collaboratively adding new knowledge to an LLM | Reports full-parameter fine-tuning as more susceptible to catastrophic forgetting compared to LORA across various conditions. Limited experimental scope warrants further verification. IBM research contribution. |
| DFlow: Diverse Dialogue Flow Simulation with Large Language Models | Proposes a scalable synthetic data approach generating diverse multi-turn dialogues following predetermined paths/trees, adhering to task logic and constraints to enhance dialogue comprehension. |
| How to Evaluate Reward Models for RLHF | Introduces significant RewardBench framework. Distribution analysis recommended. |
| Truncated Consistency Models | Demonstrates improved generation quality through reduced early-stage denoising in diffusion models. Robustness of non-trivial function preservation claims requires further validation. |
| Lossless KV Cache Compression to 2% | CLA implementation pending further analysis. |
| Mitigating Forgetting in LLM Supervised Fine-Tuning and Preference Learning | Theoretical analysis and experimental validation demonstrate sub-optimality of sequential SFT+DPO training, proposing two effective joint training methodologies. |
| SPA-Bench | Comprehensive smartphone agent evaluation benchmark. Anticipated precursor to similar frameworks. |
| Polymath: A Challenging Multi-modal Mathematical Reasoning Benchmark | Notable for interesting image classification definitions in Table 1. |
| OpenMU: Your Swiss Army Knife for Music Understanding | Notable absence of MERT implementation. |
| Automated Proof Generation for Rust Code via Self-Evolution | Valuable framework addressing data scarcity in automated Rust code proof generation. Potential application for enhancing model correction capabilities similar to CriticGPT. |
| Pre-training Distillation for Large Language Models: A Design Space Exploration | Significant research direction in pre-training distillation. Analysis pending. |
| Compute-Constrained Data Selection | Rush's work formalizes SFT data selection as a cost-aware utility function. Suggests trend toward interdisciplinary modeling approaches. Conclusions regarding complexity-based methods require further validation, particularly regarding distribution versus individual data quality impacts. |
| Self-Explained Keywords Empower Large Language Models for Code Generation | Identifies LLM limitations in extracting and explaining low-frequency keywords in problem descriptions. |
| TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling | Combines MCTS and BoN, maintaining parent nodes during sampling while iteratively branching and pruning low-quality responses to reduce computational overhead. Suggests potential benefits in returning to DAG-based reasoning forms. |
</table>

</details>
<hr/>

If you are intereted in the work published by us, please navigate to our [full paper list](https://huggingface.co/collections/m-a-p/m-a-p-full-paper-list-65e070a694c7b01c5547fbff).
