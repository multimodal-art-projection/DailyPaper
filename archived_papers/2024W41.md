<h1 align="center"><img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg" width="25"> M-A-P Daily Paper</h1>
<p align="center">
<a href="https://github.com/DenverCoder1/readme-typing-svg"><img src="https://media.giphy.com/media/Rn26lWjqA0uUU/giphy.gif" width="100"></a>
</p>
<hr/>
<h4 align="center">The <a href=https://m-a-p.ai>M-A-P</a> daily paper project curates and reviews a selection of new papers published daily on arXiv, providing insightful commentary on cutting-edge research across various scientific disciplines.</h4>
<br>
<hr/>

[back to main page](https://m-a-p.ai/DailyPaper)

## üõ†Ô∏è Papers This Week 

(Expand to View)

<details>
<summary> <b>11/10/2024</b> </summary>

<table class="center">

| Paper  | Affiliation | Comments |
|:-------------|:-------------|:-------------|
| Zero-Shot Generalization of Vision-Based RL Without Data Augmentation | University of Southern California | This paper explores the generalization of vision-based agents in new environments (specifically across game environments), which is a highly valuable long-term research topic. Humans can quickly adapt to playing games by reading guides and practicing, while models often require time for training or rely on strong "external knowledge" supervision. The common approach involves using some form of a memory module. This memory can be trained through disentangled vision representations (as shown in this paper), or based on text or a well-defined shared state space (such as controller inputs). About a year ago, work was done on this topic, including projects like MORE-3S: Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces and Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction. Earlier, Google had also worked on Multi-game Decision Transformers, though all of these are based on Decision Transformers (DT). It is believed that this could also be explored using multi-modal language models (MLLM), though significant progress has not been seen in this area for over a year, which is somewhat disappointing. |
| Can Transformers Reason Logically? A Study in SAT Solving | Georgia Institute of Technology, Google Research | This is an interesting small experiment, using SAT to show that LLMs combined with Chain-of-Thought (CoT) reasoning can generalize to out-of-distribution (OOD) cases within the same task. However, it doesn‚Äôt demonstrate how well models handle task-level OOD generalization. The mechanical interpretability aspect isn‚Äôt very solid, but the main takeaway is that the format and length of supervised fine-tuning (SFT) data are crucial. The rest of the findings are not as impactful. |
| DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models | Institute of Automation (CAS), University of California | This paper presents a data analysis agent benchmark. The benchmarks listed in Table 1 are worth considering for potential integration into specialized areas of code evaluation. This paper is recommended for review. |
| MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts | Peking University, Peng Cheng Laboratory | This method introduces zero-experts and copy-experts, though it doesn‚Äôt fully qualify as a heterogeneous Mixture-of-Experts (MoE) model since the sizes appear uniform. Further evaluation is needed to assess its true value. |
| The Moral Turing Test: Evaluating Human-LLM Alignment in Moral Decision-Making | University of Geneva, Google DeepMind | A new moral dilemma benchmark developed by GDM. |
| A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models | Duke University, Johns Hopkins University | This is a survey introducing hardware-based model architecture design. It was quickly scanned and seems decent but requires more in-depth study. |
| Agent S: An Open Agentic Framework that Uses Computers Like a Human | Simular Research | This paper introduces a GUI agent framework. A brief scan didn‚Äôt reveal anything particularly innovative. |
| Executing Arithmetic: Fine-Tuning Large Language Models as Turing Machines | Nanjing University | This is a fascinating paper, featuring an interesting design in which the authors built a pseudo-Turing machine capable of performing seven types of arithmetic operations. The key achievement is that it works well. If the design and memory capabilities can be extended, this could potentially serve as another route for inference time scaling. In fact, the accuracy is quite high. If LLM agents can be made reliable, the command set could be defined as finite, consisting of Chain-of-Thought (CoT) operations alongside a few essential actions like determining when to stop or shortening the history context. This paper is highly recommended for reading and may inspire new research directions. |
| The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks | Google Research, Google DeepMind | This paper presents a potentially valuable out-of-distribution (OOD) benchmark similar to IQ tests. |
| COMMA: A Communicative Multimodal Multi-Agent Benchmark | University of Wisconsin, Nanjing University | This benchmark evaluates multimodal multi-agent frameworks through interaction. The research direction holds significant potential, though it might become less impactful if too many similar puzzles flood the field in the future. |
| WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents | University of Technology Sydney, University of Maryland, Tencent | This work is somewhat similar to frameworks based on potential rules. However, the rule format feels trivial, and the limited action space makes the work seem unimpressive compared to other agents based on the Minecraft environment. |

</table>

</details>

<details>
<summary> <b>10/10/2024</b> </summary>

| Paper  | Main Affiliation | Comments |
| ------------- | ------------- | ------------- |
| x | y | z|

</details>

<details>
<summary> <b>09/10/2024</b> </summary>

| Paper  | Main Affiliation | Comments |
| ------------- | ------------- | ------------- |
| x | y | z|

</details>

<hr/>

If you are intereted in the work published by us, please navigate to our [full paper list](https://huggingface.co/collections/m-a-p/m-a-p-full-paper-list-65e070a694c7b01c5547fbff).
